# Ejercicio 02: Pipeline ETL Profesional - Quality of Government

> **Nivel:** Avanzado
> **DuraciÃ³n estimada:** 15-20 horas
> **Modalidad:** Grupal (2 grupos) o Individual

---

## ğŸ¯ Objetivos de Aprendizaje

Al completar este ejercicio serÃ¡s capaz de:

- âœ… DiseÃ±ar e implementar un pipeline ETL profesional
- âœ… Trabajar con PostgreSQL para anÃ¡lisis de datos longitudinales
- âœ… Limpiar y transformar datasets complejos (1000+ variables)
- âœ… Preparar datos en formato panel para anÃ¡lisis economÃ©trico
- âœ… Aplicar buenas prÃ¡cticas de ingenierÃ­a de software
- âœ… Escribir cÃ³digo modular, testeable y documentado
- âœ… Trabajar con datos reales de investigaciÃ³n acadÃ©mica

---

## ğŸ“Š Dataset: Quality of Government (QoG)

**Â¿QuÃ© es QoG?**

El Quality of Government Standard Dataset es una base de datos longitudinal mantenida por la Universidad de Gotemburgo que agrega mÃ¡s de **1000 variables** de mÃºltiples fuentes sobre calidad institucional, democracia, desarrollo econÃ³mico y social.

**CaracterÃ­sticas:**
- **Cobertura temporal:** 1946-2023 (segÃºn variable)
- **Cobertura geogrÃ¡fica:** 194+ paÃ­ses
- **Variables:** 1289 en la Ãºltima versiÃ³n
- **Fuentes:** World Bank, V-Dem, Transparency International, Freedom House, UNDP, etc.

**Usos:**
- InvestigaciÃ³n acadÃ©mica en ciencia polÃ­tica y economÃ­a
- AnÃ¡lisis comparativo de paÃ­ses
- Estudios de desarrollo institucional
- Regresiones de datos de panel

**Descargar:** https://www.qogdata.pol.gu.se/data/qog_std_ts_jan23.csv

---

## ğŸ­ Temas de AnÃ¡lisis

Debes elegir **UNO** de los siguientes temas:

### Tema 1: EvoluciÃ³n Institucional Post-Autoritaria

**Pregunta de investigaciÃ³n:**
Â¿CÃ³mo evoluciona la calidad institucional en paÃ­ses que transitan desde regÃ­menes autoritarios?

**Aspectos a analizar:**
- Trayectorias de democratizaciÃ³n
- RelaciÃ³n entre desarrollo econÃ³mico y calidad democrÃ¡tica
- Factores que explican Ã©xitos o fracasos en transiciones
- AnÃ¡lisis comparativo por regiones

**Variables clave:**
- Ãndices de democracia (V-Dem, Polity, Freedom House)
- Calidad institucional (Transparency International, World Bank Governance)
- Desarrollo econÃ³mico (PIB, HDI, Gini)

**Ver:** `especificaciones/VARIABLES_TEMA1.md`

---

### Tema 2: Recursos Naturales y Desarrollo Regional

**Pregunta de investigaciÃ³n:**
Â¿La dependencia de recursos naturales afecta el desarrollo econÃ³mico e institucional?

**Aspectos a analizar:**
- "MaldiciÃ³n de los recursos" (resource curse)
- RelaciÃ³n entre hidrocarburos y corrupciÃ³n
- Acceso a servicios bÃ¡sicos (agua, saneamiento)
- Dependencia agrÃ­cola y desarrollo

**Variables clave:**
- ProducciÃ³n de petrÃ³leo/gas (Ross dataset)
- Rentas de recursos naturales (World Bank)
- Acceso a agua y saneamiento
- Calidad institucional

**Ver:** `especificaciones/VARIABLES_TEMA2.md`

---

## ğŸ—ï¸ Arquitectura del Proyecto

Este ejercicio requiere una arquitectura **modular y profesional**.

**Estructura esperada:**
```
tu_apellido_nombre/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database/          # ConexiÃ³n PostgreSQL
â”‚   â”œâ”€â”€ etl/               # Extract, Transform, Load
â”‚   â”œâ”€â”€ analysis/          # AnÃ¡lisis de datos
â”‚   â””â”€â”€ utils/             # Utilidades (logging, etc.)
â”œâ”€â”€ scripts/               # Scripts ejecutables
â”œâ”€â”€ sql/                   # Queries SQL
â”œâ”€â”€ tests/                 # Tests (opcional)
â””â”€â”€ docs/                  # DocumentaciÃ³n
```

**Ver detalles completos:** `especificaciones/ARQUITECTURA.md`

---

## ğŸ”§ TecnologÃ­as Requeridas

### Obligatorias

- **Python 3.11+**
- **PostgreSQL 14+** (instalaciÃ³n local)
- **pandas** - ManipulaciÃ³n de datos
- **psycopg2** - ConexiÃ³n PostgreSQL
- **python-dotenv** - Variables de entorno

### Opcionales pero Recomendadas

- **SQLAlchemy** - ORM para PostgreSQL
- **pytest** - Testing
- **logging** - Sistema de logs
- **click** - CLIs elegantes

---

## ğŸ“‹ Tareas a Realizar

### Fase 0: Setup

1. Instalar PostgreSQL (ver `docs/POSTGRESQL_SETUP.md`)
2. Crear base de datos `qog_research`
3. Ejecutar `especificaciones/ESQUEMA_DB.sql`
4. Configurar variables de entorno (`.env`)

### Fase 1: Extract (ETL - E)

**MÃ³dulo:** `src/etl/extract.py`

**Tareas:**
- Descargar dataset QoG desde URL oficial
- Implementar cachÃ© local (no descargar cada vez)
- Filtrar por tema elegido (1 o 2)
- Filtrar por perÃ­odo temporal (ej: 1990-2023)
- Validar integridad del dataset

**Funciones a implementar:**
- `download_qog_data()`
- `filter_by_theme()`
- `validate_data_quality()`

**Ver:** `especificaciones/FUNCIONES_REQUERIDAS.md`

---

### Fase 2: Transform (ETL - T)

**MÃ³dulo:** `src/etl/transform.py`

**Tareas:**
- Renombrar columnas crÃ­pticas a nombres legibles
- Crear variables derivadas (deciles, categorÃ­as, Ã­ndices)
- Manejar valores faltantes estratÃ©gicamente
- Detectar y corregir outliers (si aplica)
- Normalizar formatos de datos

**Funciones a implementar:**
- `rename_columns()`
- `create_derived_variables()`
- `handle_missing_values()`

**Consideraciones:**
- NO eliminar paÃ­ses sin recursos (son grupo de control)
- Valores faltantes: forward-fill **dentro de paÃ­s** (no entre paÃ­ses)
- Logging detallado de cada transformaciÃ³n

---

### Fase 3: Load (ETL - L)

**MÃ³dulo:** `src/etl/load.py`

**Tareas:**
- Conectar a PostgreSQL con manejo de errores
- Cargar datos a tabla `qog_data` eficientemente
- Cargar catÃ¡logo de paÃ­ses a tabla `countries`
- Insertar metadata de variables
- Verificar integridad referencial

**Funciones a implementar:**
- `create_connection()`
- `load_to_postgres()`

**Optimizaciones:**
- Usar batches para inserciones grandes
- Transactions para atomicidad
- Logging de progreso

---

### Fase 4: Analysis

**MÃ³dulo:** `src/analysis/panel_data.py`

**Tareas:**
- Preparar panel balanceado (paÃ­ses con datos completos)
- Calcular estadÃ­sticas descriptivas por regiÃ³n/aÃ±o
- Exportar datos limpios para anÃ¡lisis:
  - CSV para Python/R
  - .dta para Stata
  - Parquet para Big Data
- Generar reporte de calidad de datos

**Funciones a implementar:**
- `prepare_panel_data()`
- `export_for_stata()`
- `generate_summary_stats()`

---

### Fase 5: Scripts Ejecutables

**Scripts en:** `scripts/`

**Requeridos:**

1. **`setup_database.py`**
   - Crear schema PostgreSQL
   - Verificar conexiÃ³n
   - Inicializar tablas

2. **`run_etl.py`**
   - Pipeline completo: Extract â†’ Transform â†’ Load
   - Argumentos: `--tema 1`, `--year-start 2000`, `--year-end 2020`
   - Logging completo de progreso

3. **`generate_report.py`**
   - Generar reporte de anÃ¡lisis
   - EstadÃ­sticas descriptivas
   - Export a CSV/Excel

**Ejemplo de uso:**
```bash
python scripts/setup_database.py
python scripts/run_etl.py --tema 1 --year-start 1990
python scripts/generate_report.py --tema 1 --output reports/
```

---

### Fase 6: SQL Avanzado

**UbicaciÃ³n:** `sql/`

**Tareas:**
- Escribir queries complejas en archivos .sql
- Usar las vistas creadas en el esquema
- AnÃ¡lisis con CTEs, window functions, agregaciones

**Ejemplos de queries:**

**Para Tema 1:**
```sql
-- EvoluciÃ³n de democracia por regiÃ³n
-- Comparativa: paÃ­ses que democratizaron vs que no
-- RelaciÃ³n PIB y calidad institucional
```

**Para Tema 2:**
```sql
-- Top 10 paÃ­ses dependientes de recursos
-- CorrelaciÃ³n recursos-corrupciÃ³n
-- EvoluciÃ³n acceso a agua por regiÃ³n
```

---

## ğŸ“š Especificaciones TÃ©cnicas

**Lee TODOS estos documentos antes de empezar:**

1. **`especificaciones/ARQUITECTURA.md`**
   - Estructura de carpetas esperada
   - SeparaciÃ³n de responsabilidades
   - Buenas prÃ¡cticas

2. **`especificaciones/ESQUEMA_DB.sql`**
   - Schema PostgreSQL completo
   - Tablas, vistas, funciones
   - Comentarios explicativos

3. **`especificaciones/FUNCIONES_REQUERIDAS.md`**
   - Firmas de funciones (input/output)
   - Comportamiento esperado
   - Ejemplos de uso

4. **`especificaciones/VARIABLES_TEMA*.md`**
   - Variables sugeridas por tema
   - Prompts para investigar mÃ¡s
   - Recursos bibliogrÃ¡ficos

5. **`especificaciones/VALIDACIONES.md`**
   - Checks de calidad obligatorios
   - Asserts y excepciones
   - Logging requerido

---

## ğŸ“ Datos de Panel - InformaciÃ³n Importante

Este ejercicio te prepara para **anÃ¡lisis economÃ©trico con datos de panel**.

**Â¿QuÃ© es panel data?**

```
Panel = Cross-section (paÃ­ses) Ã— Time-series (aÃ±os)

| country | year | var1 | var2 |
|---------|------|------|------|
| ESP     | 2000 | 100  | 50   |
| ESP     | 2001 | 102  | 51   |
| USA     | 2000 | 200  | 80   |
| USA     | 2001 | 205  | 82   |
```

**Â¿Para quÃ© sirve?**
- Controlar heterogeneidad no observada (Fixed Effects)
- Estudiar efectos causales (Difference-in-Differences)
- Mayor poder estadÃ­stico
- Modelar dinÃ¡micas temporales

**Tu pipeline debe:**
- Asegurar (paÃ­s, aÃ±o) sea Ãºnico
- Mantener datos balanceados (mismos aÃ±os para todos)
- Facilitar export a Stata/R para regresiones

---

## âœ… Criterios de EvaluaciÃ³n

### Funcionalidad (40%)
- Pipeline ejecuta sin errores
- Datos se cargan correctamente a PostgreSQL
- Resultados son correctos
- Scripts CLI funcionan

### Arquitectura (25%)
- SeparaciÃ³n de responsabilidades (ETL, DB, anÃ¡lisis)
- CÃ³digo modular y reutilizable
- Manejo de errores robusto
- ConfiguraciÃ³n externalizada

### Calidad de CÃ³digo (20%)
- PEP 8 / Black formatting
- Type hints en funciones
- Docstrings completos
- Nombres descriptivos

### DocumentaciÃ³n (10%)
- README claro
- Comentarios donde necesario
- Instrucciones de uso
- ExplicaciÃ³n de decisiones

### InnovaciÃ³n (5%)
- Tests automatizados
- Visualizaciones
- AnÃ¡lisis adicionales
- Optimizaciones

---

## ğŸ“¦ Entregables

**Carpeta de entrega:** `entregas/02_limpieza_datos/tu_apellido_nombre/`

**Estructura mÃ­nima:**
```
apellido_nombre/
â”œâ”€â”€ README.md                    # DocumentaciÃ³n proyecto
â”œâ”€â”€ requirements.txt             # Dependencias
â”œâ”€â”€ .env.example                 # Variables entorno (ejemplo)
â”œâ”€â”€ src/                         # CÃ³digo fuente
â”œâ”€â”€ scripts/                     # Scripts ejecutables
â”œâ”€â”€ sql/                         # Queries SQL
â”œâ”€â”€ docs/                        # DocumentaciÃ³n adicional
â””â”€â”€ METODOLOGIA.md               # Decisiones de diseÃ±o
```

**NO incluir:**
- Datos crudos (data/raw/)
- Archivos .db o dumps PostgreSQL
- Logs (logs/)
- Virtual environments (venv/, .venv/)
- __pycache__/

---

## ğŸš€ CÃ³mo Empezar

### 1. Lee TODA la documentaciÃ³n
No empieces a codear antes de entender el alcance completo.

### 2. Setup del entorno
```bash
# Crear carpeta
mkdir apellido_nombre
cd apellido_nombre

# Virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Instalar PostgreSQL (ver docs/POSTGRESQL_SETUP.md)

# Crear base de datos
psql -U postgres
CREATE DATABASE qog_research;
\c qog_research
-- Ejecutar especificaciones/ESQUEMA_DB.sql
```

### 3. Investigar variables
- Lee el codebook QoG
- Usa los prompts en VARIABLES_TEMA*.md
- Selecciona variables para tu tema

### 4. Implementar paso a paso
- Empieza por Extract
- Luego Transform
- Luego Load
- Finalmente Analysis

### 5. Testear frecuentemente
No esperes a tener todo para probar.

---

## ğŸ†˜ Recursos de Apoyo

### Dataset QoG
- **Website:** https://www.qog.pol.gu.se/
- **Codebook:** https://www.qogdata.pol.gu.se/data/codebook_std_jan23.pdf
- **Download:** https://www.qogdata.pol.gu.se/data/qog_std_ts_jan23.csv

### PostgreSQL
- **DocumentaciÃ³n:** https://www.postgresql.org/docs/
- **Tutorial:** https://www.postgresqltutorial.com/

### Python Libraries
- **pandas:** https://pandas.pydata.org/docs/
- **psycopg2:** https://www.psycopg.org/docs/
- **SQLAlchemy:** https://docs.sqlalchemy.org/

### Panel Data
- **linearmodels:** https://bashtage.github.io/linearmodels/
- **Stata export:** pandas.DataFrame.to_stata()

---

## â“ Preguntas Frecuentes

**P: Â¿Tengo que usar TODAS las variables sugeridas?**
R: No. Son SUGERENCIAS. Investiga y elige las mÃ¡s relevantes para tu anÃ¡lisis.

**P: Â¿Puedo cambiar de tema a mitad del proyecto?**
R: SÃ­, pero perderÃ¡s tiempo. Elige bien desde el inicio.

**P: Â¿El proyecto es individual o grupal?**
R: Puedes elegir. Grupos de 4-5 o individual.

**P: Â¿CuÃ¡ntas filas/paÃ­ses debo analizar?**
R: MÃ­nimo 50 paÃ­ses, 20 aÃ±os (1000+ observaciones).

**P: Â¿Tengo que implementar TODO lo que estÃ¡ en FUNCIONES_REQUERIDAS.md?**
R: Las funciones bÃ¡sicas sÃ­. Las avanzadas son opcionales.

**P: Â¿Puedo usar Docker?**
R: Para este ejercicio NO. PostgreSQL local. Docker serÃ¡ en ejercicios futuros.

**P: Â¿QuÃ© hago si no encuentro una variable en QoG?**
R: Usa los prompts en VARIABLES_TEMA*.md para buscar alternativas.

---

## ğŸ¯ Consejos Finales

1. **Empieza simple:** Pipeline bÃ¡sico primero, optimizaciones despuÃ©s
2. **Logging es tu amigo:** Log todo, te salvarÃ¡ en debugging
3. **Git desde dÃ­a 1:** Commits frecuentes y descriptivos
4. **Documenta mientras codeas:** README no es lo Ãºltimo
5. **Pregunta temprano:** Si algo no estÃ¡ claro, pregunta
6. **Lee el codebook QoG:** Es tu biblia para este ejercicio
7. **Testea con datos pequeÃ±os:** No cargues 1M de filas de golpe

---

## ğŸ”® PreparaciÃ³n para Docker (Futuro)

Este proyecto estÃ¡ diseÃ±ado para ser **dockerizado** en ejercicios futuros.

Tu arquitectura modular facilitarÃ¡:
- Contenedor PostgreSQL
- Contenedor aplicaciÃ³n Python
- docker-compose para orquestar

**Por ahora:** PostgreSQL local es suficiente.

---

**Â¡Buena suerte!** ğŸš€

Este ejercicio es desafiante pero te darÃ¡ habilidades de nivel profesional.

---

**Creado:** 2025-12-17
**Ãšltima actualizaciÃ³n:** 2025-12-17
